{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(s):\n",
    "    s = re.sub(r\"[^A-Za-z0-9:(),!?\\'\\`]\", \" \", s)\n",
    "    s = re.sub(r\" : \", \":\", s)\n",
    "    s = re.sub(r\"\\'s\", \" \\'s\", s)\n",
    "    s = re.sub(r\"\\'ve\", \" \\'ve\", s)\n",
    "    s = re.sub(r\"n\\'t\", \" n\\'t\", s)\n",
    "    s = re.sub(r\"\\'re\", \" \\'re\", s)\n",
    "    s = re.sub(r\"\\'d\", \" \\'d\", s)\n",
    "    s = re.sub(r\"\\'ll\", \" \\'ll\", s)\n",
    "    s = re.sub(r\",\", \" , \", s)\n",
    "    s = re.sub(r\"!\", \" ! \", s)\n",
    "    s = re.sub(r\"\\(\", \" \\( \", s)\n",
    "    s = re.sub(r\"\\)\", \" \\) \", s)\n",
    "    s = re.sub(r\"\\?\", \" \\? \", s)\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "    return s.strip().lower()\n",
    "\n",
    "def load_embeddings(vocabulary):\n",
    "    word_embeddings = {}\n",
    "    for word in vocabulary:\n",
    "        word_embeddings[word] = np.random.uniform(-0.25, 0.25, 300)\n",
    "    return word_embeddings\n",
    "\n",
    "def pad_sentences(sentences, padding_word=\"<PAD/>\", forced_sequence_length=None):\n",
    "    \"\"\"Pad setences during training or prediction\"\"\"\n",
    "    if forced_sequence_length is None: # Train\n",
    "        sequence_length = max(len(x) for x in sentences)\n",
    "    else: # Prediction\n",
    "        logging.critical('This is prediction, reading the trained sequence length')\n",
    "        sequence_length = forced_sequence_length\n",
    "    logging.critical('The maximum length is {}'.format(sequence_length))\n",
    "\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "\n",
    "        if num_padding < 0: # Prediction: cut off the sentence if it is longer than the sequence length\n",
    "            logging.info('This sentence has to be cut off because it is longer than trained sequence length')\n",
    "            padded_sentence = sentence[0:sequence_length]\n",
    "        else:\n",
    "            padded_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(padded_sentence)\n",
    "    return padded_sentences\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    vocabulary_inv = [word[0] for word in word_counts.most_common()]\n",
    "    vocabulary = {word: index for index, word in enumerate(vocabulary_inv)}\n",
    "    return vocabulary, vocabulary_inv\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int(data_size / batch_size) + 1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "\n",
    "def load_data(filename):\n",
    "    df = pd.read_csv(filename, compression='zip')\n",
    "    selected = ['Category', 'Descript']\n",
    "    non_selected = list(set(df.columns) - set(selected))\n",
    "\n",
    "    df = df.drop(non_selected, axis=1)\n",
    "    df = df.dropna(axis=0, how='any', subset=selected)\n",
    "    df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "    labels = sorted(list(set(df[selected[0]].tolist())))\n",
    "    num_labels = len(labels)\n",
    "    one_hot = np.zeros((num_labels, num_labels), int)\n",
    "    np.fill_diagonal(one_hot, 1)\n",
    "    label_dict = dict(zip(labels, one_hot))\n",
    "\n",
    "    x_raw= df[selected[1]].apply(lambda x: clean_str(x).split(' ')).tolist()\n",
    "    y_raw = df[selected[0]].apply(lambda y: label_dict[y]).tolist()\n",
    "\n",
    "    x_raw = pad_sentences(x_raw)\n",
    "    vocabulary, vocabulary_inv = build_vocab(x_raw)\n",
    "\n",
    "    x = np.array([[vocabulary[word] for word in sentence] for sentence in x_raw])\n",
    "    y = np.array(y_raw)\n",
    "    return x, y, vocabulary, vocabulary_inv, df, labels\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_file = './data/train.csv.zip'\n",
    "    load_data(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.n_classes = n_classes = config.n_classes\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        maxSeqLength = config.maxSeqLength\n",
    "        size = config.hidden_size\n",
    "        vocab_size = config.vocab_size\n",
    "        self.l2_reg_lambda=0.0\n",
    "        #self.is_training = is_training\n",
    "        self.l2_loss = tf.constant(0.0)\n",
    "\n",
    "        self._input_data = tf.placeholder(tf.int32, [None, maxSeqLength], name=\"input_x\")\n",
    "        self._target = tf.placeholder(tf.float32, [None, n_classes], name=\"input_y\")\n",
    "\n",
    "        #lstm_cell = rnn.BasicLSTMCell(size, forget_bias=0.0, state_is_tuple=True)\n",
    "        lstm_cell = rnn.GRUCell(num_units=size)\n",
    "\n",
    "        lstm_cell = rnn.DropoutWrapper(lstm_cell, output_keep_prob=self.dropout_keep_prob)\n",
    "        cell = rnn.MultiRNNCell([lstm_cell] * config.num_layers, state_is_tuple=True)\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        #initial_state = lstm_cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            #embedding = tf.get_variable(\"embedding\", [vocab_size, size], dtype=tf.float32)\n",
    "            embedding = tf.Variable(tf.random_uniform([vocab_size, size],-1.0,1.0),name=\"embedding\")\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self._input_data)\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            inputs = tf.nn.dropout(inputs, self.dropout_keep_prob)\n",
    "\n",
    "        output, state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state)\n",
    "        #output, states = tf.contrib.rnn.static_rnn(lstm_cell, inputs, dtype=tf.float32, initial_state=initial_state,sequence_length=maxSeqLength)\n",
    "        #output = tf.stack(output)\n",
    "        output = tf.transpose(output, [1, 0, 2])\n",
    "        last = tf.gather(output, int(output.get_shape()[0]) - 1)\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [size, n_classes], dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer())\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [n_classes], dtype=tf.float32)\n",
    "        self.l2_loss += tf.nn.l2_loss(softmax_w)\n",
    "        self.l2_loss += tf.nn.l2_loss(softmax_b)\n",
    "        logits = tf.nn.xw_plus_b(last,softmax_w,softmax_b,name=\"score\")\n",
    "\n",
    "        self._cost = cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self._target))+self.l2_reg_lambda+self.l2_loss\n",
    "\n",
    "        self._final_state = state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, embedding_mat, non_static, hidden_unit, sequence_length,\n",
    "        num_classes, embedding_size):\n",
    "\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name='input_y')\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')\n",
    "        self.batch_size = tf.placeholder(tf.int32, [])\n",
    "        self.pad = tf.placeholder(tf.float32, [None, 1, embedding_size, 1], name='pad')\n",
    "        self.real_len = tf.placeholder(tf.int32, [None], name='real_len')\n",
    "\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        with tf.device('/cpu:0'), tf.name_scope('embedding'):\n",
    "            if not non_static:\n",
    "                W = tf.constant(embedding_mat, name='W')\n",
    "            else:\n",
    "                W = tf.Variable(embedding_mat, name='W')\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            emb = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        pooled_concat = []\n",
    "        reduced = np.int32(np.ceil((sequence_length) * 1.0 / max_pool_size))\n",
    "\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope('conv-maxpool-%s' % filter_size):\n",
    "\n",
    "                # Zero paddings so that the convolution output have dimension batch x sequence_length x emb_size x channel\n",
    "                num_prio = (filter_size-1) // 2\n",
    "                num_post = (filter_size-1) - num_prio\n",
    "                pad_prio = tf.concat([self.pad] * num_prio,1)\n",
    "                pad_post = tf.concat([self.pad] * num_post,1)\n",
    "                emb_pad = tf.concat([pad_prio, emb, pad_post],1)\n",
    "\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name='W')\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name='b')\n",
    "                conv = tf.nn.conv2d(emb_pad, W, strides=[1, 1, 1, 1], padding='VALID', name='conv')\n",
    "\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name='relu')\n",
    "\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(h, ksize=[1, max_pool_size, 1, 1], strides=[1, max_pool_size, 1, 1], padding='SAME', name='pool')\n",
    "                pooled = tf.reshape(pooled, [-1, reduced, num_filters])\n",
    "                pooled_concat.append(pooled)\n",
    "\n",
    "        pooled_concat = tf.concat(pooled_concat,2)\n",
    "        pooled_concat = tf.nn.dropout(pooled_concat, self.dropout_keep_prob)\n",
    "\n",
    "        # lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_unit)\n",
    "\n",
    "        #lstm_cell = tf.nn.rnn_cell.GRUCell(num_units=hidden_unit)\n",
    "        lstm_cell = tf.contrib.rnn.GRUCell(num_units=hidden_unit)\n",
    "\n",
    "        #lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=self.dropout_keep_prob)\n",
    "        lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=self.dropout_keep_prob)\n",
    "\n",
    "\n",
    "        self._initial_state = lstm_cell.zero_state(self.batch_size, tf.float32)\n",
    "        #inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(1, reduced, pooled_concat)]\n",
    "        inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(pooled_concat,num_or_size_splits=int(reduced),axis=1)]\n",
    "        #outputs, state = tf.nn.rnn(lstm_cell, inputs, initial_state=self._initial_state, sequence_length=self.real_len)\n",
    "        outputs, state = tf.contrib.rnn.static_rnn(lstm_cell, inputs, initial_state=self._initial_state, sequence_length=self.real_len)\n",
    "\n",
    "        # Collect the appropriate last words into variable output (dimension = batch x embedding_size)\n",
    "        output = outputs[0]\n",
    "        with tf.variable_scope('Output'):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            one = tf.ones([1, hidden_unit], tf.float32)\n",
    "            for i in range(1,len(outputs)):\n",
    "                ind = self.real_len < (i+1)\n",
    "                ind = tf.to_float(ind)\n",
    "                ind = tf.expand_dims(ind, -1)\n",
    "                mat = tf.matmul(ind, one)\n",
    "                output = tf.add(tf.multiply(output, mat),tf.multiply(outputs[i], 1.0 - mat))\n",
    "\n",
    "        with tf.name_scope('output'):\n",
    "            self.W = tf.Variable(tf.truncated_normal([hidden_unit, num_classes], stddev=0.1), name='W')\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name='b')\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(output, self.W, b, name='scores')\n",
    "            self.predictions = tf.argmax(self.scores, 1, name='predictions')\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(labels = self.input_y, logits = self.scores) #  only named arguments accepted            \n",
    "            self.loss = t`f.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name='accuracy')\n",
    "\n",
    "        with tf.name_scope('num_correct'):\n",
    "            correct = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "\n",
    "            self.num_correct = tf.reduce_sum(tf.cast(correct, 'float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import pickle\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "def train_rnn():\n",
    "    input_file = sys.argv[1]\n",
    "    x_, y_, vocabulary, vocabulary_inv, df, labels = data_helper.load_data(input_file)\n",
    "\n",
    "    training_config = sys.argv[2]\n",
    "    params = json.loads(open(training_config).read())\n",
    "\n",
    "    # Assign a 300 dimension vector to each word\n",
    "    word_embeddings = data_helper.load_embeddings(vocabulary)\n",
    "    embedding_mat = [word_embeddings[word] for index, word in enumerate(vocabulary_inv)]\n",
    "    embedding_mat = np.array(embedding_mat, dtype = np.float32)\n",
    "\n",
    "    # Split the original dataset into train set and test set\n",
    "    x, x_test, y, y_test = train_test_split(x_, y_, test_size=0.1)\n",
    "\n",
    "    # Split the train set into train set and dev set\n",
    "    x_train, x_dev, y_train, y_dev = train_test_split(x, y, test_size=0.1)\n",
    "\n",
    "    logging.info('x_train: {}, x_dev: {}, x_test: {}'.format(len(x_train), len(x_dev), len(x_test)))\n",
    "    logging.info('y_train: {}, y_dev: {}, y_test: {}'.format(len(y_train), len(y_dev), len(y_test)))\n",
    "\n",
    "    # Create a directory, everything related to the training will be saved in this directory\n",
    "    timestamp = str(int(time.time()))\n",
    "    trained_dir = './trained_results_' + timestamp + '/'\n",
    "    if os.path.exists(trained_dir):\n",
    "        shutil.rmtree(trained_dir)\n",
    "    os.makedirs(trained_dir)\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn_rnn = TextCNNRNN(\n",
    "                embedding_mat=embedding_mat,\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes = y_train.shape[1],\n",
    "                non_static=params['non_static'],\n",
    "                hidden_unit=params['hidden_unit'],\n",
    "                max_pool_size=params['max_pool_size'],\n",
    "                filter_sizes=map(int, params['filter_sizes'].split(\",\")),\n",
    "                num_filters = params['num_filters'],\n",
    "                embedding_size = params['embedding_dim'],\n",
    "                l2_reg_lambda = params['l2_reg_lambda'])\n",
    "\n",
    "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            optimizer = tf.train.RMSPropOptimizer(1e-3, decay=0.9)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn_rnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Checkpoint files will be saved in this directory during training\n",
    "            checkpoint_dir = './checkpoints_' + timestamp + '/'\n",
    "            if os.path.exists(checkpoint_dir):\n",
    "                shutil.rmtree(checkpoint_dir)\n",
    "            os.makedirs(checkpoint_dir)\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, 'model')\n",
    "\n",
    "            def real_len(batches):\n",
    "                return [np.ceil(np.argmin(batch + [0]) * 1.0 / params['max_pool_size']) for batch in batches]\n",
    "\n",
    "            def train_step(x_batch, y_batch):\n",
    "                feed_dict = {\n",
    "                    cnn_rnn.input_x: x_batch,\n",
    "                    cnn_rnn.input_y: y_batch,\n",
    "                    cnn_rnn.dropout_keep_prob: params['dropout_keep_prob'],\n",
    "                    cnn_rnn.batch_size: len(x_batch),\n",
    "                    cnn_rnn.pad: np.zeros([len(x_batch), 1, params['embedding_dim'], 1]),\n",
    "                    cnn_rnn.real_len: real_len(x_batch),\n",
    "                }\n",
    "                _, step, loss, accuracy = sess.run([train_op, global_step, cnn_rnn.loss, cnn_rnn.accuracy], feed_dict)\n",
    "\n",
    "            def dev_step(x_batch, y_batch):\n",
    "                feed_dict = {\n",
    "                    cnn_rnn.input_x: x_batch,\n",
    "                    cnn_rnn.input_y: y_batch,\n",
    "                    cnn_rnn.dropout_keep_prob: 1.0,\n",
    "                    cnn_rnn.batch_size: len(x_batch),\n",
    "                    cnn_rnn.pad: np.zeros([len(x_batch), 1, params['embedding_dim'], 1]),\n",
    "                    cnn_rnn.real_len: real_len(x_batch),\n",
    "                }\n",
    "                step, loss, accuracy, num_correct, predictions = sess.run(\n",
    "                    [global_step, cnn_rnn.loss, cnn_rnn.accuracy, cnn_rnn.num_correct, cnn_rnn.predictions], feed_dict)\n",
    "                return accuracy, loss, num_correct, predictions\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # Training starts here\n",
    "            train_batches = data_helper.batch_iter(list(zip(x_train, y_train)), params['batch_size'], params['num_epochs'])\n",
    "            best_accuracy, best_at_step = 0, 0\n",
    "\n",
    "            # Train the model with x_train and y_train\n",
    "            for train_batch in train_batches:\n",
    "                x_train_batch, y_train_batch = zip(*train_batch)\n",
    "                train_step(x_train_batch, y_train_batch)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "                # Evaluate the model with x_dev and y_dev\n",
    "                if current_step % params['evaluate_every'] == 0:\n",
    "                    dev_batches = data_helper.batch_iter(list(zip(x_dev, y_dev)), params['batch_size'], 1)\n",
    "\n",
    "                    total_dev_correct = 0\n",
    "                    for dev_batch in dev_batches:\n",
    "                        x_dev_batch, y_dev_batch = zip(*dev_batch)\n",
    "                        acc, loss, num_dev_correct, predictions = dev_step(x_dev_batch, y_dev_batch)\n",
    "                        total_dev_correct += num_dev_correct\n",
    "                    accuracy = float(total_dev_correct) / len(y_dev)\n",
    "                    logging.info('Accuracy on dev set: {}'.format(accuracy))\n",
    "\n",
    "                    if accuracy >= best_accuracy:\n",
    "                        best_accuracy, best_at_step = accuracy, current_step\n",
    "                        path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                        logging.critical('Saved model {} at step {}'.format(path, best_at_step))\n",
    "                        logging.critical('Best accuracy {} at step {}'.format(best_accuracy, best_at_step))\n",
    "            logging.critical('Training is complete, testing the best model on x_test and y_test')\n",
    "\n",
    "            # Save the model files to trained_dir. predict.py needs trained model files. \n",
    "            saver.save(sess, trained_dir + \"best_model.ckpt\")\n",
    "\n",
    "            # Evaluate x_test and y_test\n",
    "            saver.restore(sess, checkpoint_prefix + '-' + str(best_at_step))\n",
    "            test_batches = data_helper.batch_iter(list(zip(x_test, y_test)), params['batch_size'], 1, shuffle=False)\n",
    "            total_test_correct = 0\n",
    "            for test_batch in test_batches:\n",
    "                x_test_batch, y_test_batch = zip(*test_batch)\n",
    "                acc, loss, num_test_correct, predictions = dev_step(x_test_batch, y_test_batch)\n",
    "                total_test_correct += int(num_test_correct)\n",
    "            logging.critical('Accuracy on test set: {}'.format(float(total_test_correct) / len(y_test)))\n",
    "\n",
    "    # Save trained parameters and files since predict.py needs them\n",
    "    with open(trained_dir + 'words_index.json', 'w') as outfile:\n",
    "        json.dump(vocabulary, outfile, indent=4, ensure_ascii=False)\n",
    "    with open(trained_dir + 'embeddings.pickle', 'wb') as outfile:\n",
    "        pickle.dump(embedding_mat, outfile, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(trained_dir + 'labels.json', 'w') as outfile:\n",
    "        json.dump(labels, outfile, indent=4, ensure_ascii=False)\n",
    "\n",
    "    params['sequence_length'] = x_train.shape[1]\n",
    "    with open(trained_dir + 'trained_parameters.json', 'w') as outfile:\n",
    "        json.dump(params, outfile, indent=4, sort_keys=True, ensure_ascii=False)\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "    # python3 train.py ./data/train.csv.zip ./training_config.json\n",
    "        train_cnn_rnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if isinstance(INPUTS, tuple):\n",
    "    # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "    INPUTS = tf.concat(INPUTS, 2)\n",
    "\n",
    "if time_major:\n",
    "    # (T,B,D) => (B,T,D)\n",
    "    INPUTS = tf.array_ops.transpose(INPUTS, [1, 0, 2])\n",
    "\n",
    "inputs_shape = INPUTS.shape\n",
    "hidden_size = inputs_shape[2].value\n",
    "\n",
    "# Attention mechanism\n",
    "W_omega = tf.get_variable(name='W_omega', shape=[hidden_size, ATTENTION_SIZE], dtype=tf.float32,\n",
    "                          initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "b_omega = tf.get_variable(name='b_omega', shape=[ATTENTION_SIZE], dtype=tf.float32,\n",
    "                          initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "u_omega = tf.get_variable(name='u_omega', shape=[ATTENTION_SIZE], dtype=tf.float32,\n",
    "                          initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "\n",
    "v = tf.tanh(tf.matmul(tf.reshape(INPUTS, [-1, hidden_size]), W_omega) + tf.reshape(b_omega, [1, -1]))\n",
    "vu = tf.reshape(tf.matmul(v, tf.reshape(u_omega, [-1, 1])), [-1, inputs_shape[1].value])\n",
    "alphas = tf.nn.softmax(vu)\n",
    "\n",
    "output = tf.reduce_sum(INPUTS * tf.expand_dims(alphas, -1), 1)\n",
    "\n",
    "if not return_alphas:\n",
    "    return output\n",
    "else:\n",
    "    return output, alphas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
